{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3fe97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'D:\\MLOPS_dvc\\dvc_project\\data\\spam.csv',encoding='latin')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b118c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8859a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74f4eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(\n",
    "    columns = {\n",
    "        'v1' : 'target',\n",
    "        'v2' : 'text'\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759a43d",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0058b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa1ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target.value_counts().plot(\n",
    "    kind = 'bar',\n",
    "    color = ['#FA745A','#F7B7AD'],\n",
    "    legend=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1152b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Nulls in data : \\n{data.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dulicates {data.duplicated().sum()}\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d3263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(keep = \"first\",inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf537f5",
   "metadata": {},
   "source": [
    "## Feature Engg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c51cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove URLs, mentions, hashtags\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\w+|#\\w+', '', text)\n",
    "\n",
    "    # 3. Remove numbers and punctuation\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # 4. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 5. Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # 6. Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # 7. Join back to string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe716bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['transformed_text'] = data['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70347dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af79672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "data['target'] = encoder.fit_transform(data['target'])\n",
    "tfid = TfidfVectorizer(max_features = 500)\n",
    "\n",
    "X= tfid.fit_transform(data['transformed_text']).toarray()\n",
    "y = data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import   train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea365811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import   LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa4a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='sigmoid',gamma = 1.0)\n",
    "knc = KNeighborsClassifier()\n",
    "mnb = MultinomialNB()\n",
    "dtc = DecisionTreeClassifier(max_depth=5)\n",
    "lrc = LogisticRegression(solver='liblinear',penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=50,random_state=2)\n",
    "abc = AdaBoostClassifier(n_estimators=50,random_state=2)\n",
    "bc = BaggingClassifier(n_estimators=50,random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=50,random_state=2)\n",
    "gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
    "xgb = XGBClassifier(n_estimators = 50,random_state = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {\n",
    "    'SVC' : svc,\n",
    "    'KNN' : knc,\n",
    "    'NB' : mnb,\n",
    "    'DT' : dtc,\n",
    "    'LR' : lrc,\n",
    "    'RF' : rfc,\n",
    "    'Adaboost' : abc,\n",
    "    'Bgc' : bc,\n",
    "    'ETC' : etc,\n",
    "    'GBDT' : gbdt,\n",
    "    'xgb' : xgb\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510cf610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report,ConfusionMatrixDisplay\n",
    "def train_classifier (clfs,x_train,y_train,x_test,y_test):\n",
    "    clfs.fit(x_train,y_train)\n",
    "    y_pred = clfs.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    cfr = classification_report(y_test,y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return accuracy,precision,cm,cfr,clfs.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b943eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores =[]\n",
    "precision_scores = []\n",
    "cm_scores =[]\n",
    "cfr_scores = []\n",
    "for name,clfs in clfs.items():\n",
    "    current_accuracy , current_precision, c_cm,c_cfr,classes = train_classifier(clfs,x_train,y_train,x_test,y_test)\n",
    "    print()\n",
    "    print('For : ',name)\n",
    "    print('Accuracy : ' , current_accuracy)\n",
    "    print('Precision : ',current_precision)\n",
    "    print(\"Classification_report : \\n\", c_cfr )\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=c_cm, display_labels=classes)\n",
    "    disp.plot(cmap='Blues', values_format='d')\n",
    "\n",
    "    plt.title(f\"Confusion Matrix of {name}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    accuracy_scores.append(current_accuracy)\n",
    "    precision_scores.append(current_precision)\n",
    "    cm_scores.append(c_cm)\n",
    "    cfr_scores.append(c_cfr)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702fe13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
